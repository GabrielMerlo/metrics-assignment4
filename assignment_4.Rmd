---
title: |
  | Assignment 4 
  | Econometrics I
subtitle: "Universidad Carlos III de Madrid"
author: "Gabriel Merlo"
date:
header-includes: 
  - \usepackage{float}
      \floatplacement{figure}{H}
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center", tidy.opts = list(width.cutoff = 60),tidy = TRUE)
```

```{r include_packages, results = "hide"}
# install packages (if missing)
list_packages <- c("aod", "dplyr", "glmnet", "lmtest", "MASS", "pracma", "quantreg", "tidyr", "VGAM")
new_packages <- list_packages[!(list_packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)

# Load packages
sapply(list_packages, require, character.only = TRUE)
```

` `

# Part 1

## (a) Read the data and estimate the ATE using the standard difference of sample means and a linear regression using as controls X.

` ` 

```{r part_1a}
# Load data
penn <- as.data.frame(read.table("penn_jae.dat", header = TRUE))

# Keep control group, and treatment group 4
penn4 <- penn %>% filter(tg == 0 | tg == 4)

# Recode treatment variable
penn4$tg <- recode(penn4$tg, `4` = 1L) 

# Control variables
x <- c("female", "black", "othrace", "dep", "q2", "q3", "q4", "q5", "q6", "agelt35", "agegt54", "durable", "lusd", "husd")

# Log transformation of dependent variable
penn4$l_inuidur1 <- log(penn4$inuidur1)

# ATE from difference of sample means
penn4_summary <- penn4 %>% 
  group_by(tg) %>% 
  summarize(mean = mean(l_inuidur1), sd = sd(l_inuidur1), n = n())
ate_diff_mean <- penn4_summary %>%
  dplyr::select(tg, mean) %>% 
  spread(tg, mean) %>% 
  summarize(diff = `1` - `0`)
(ate_diff_mean <- as.numeric(ate_diff_mean))

# ATE from linear regression with controls
ate_ols <- lm(as.formula(paste0("l_inuidur1 ~ tg + ", paste0(x, collapse = " + "))), data = penn4)
summary(ate_ols)

# Checking balance of covariates
penn4 %>%
  group_by(tg) %>%
  dplyr::select(x) %>%
  summarise_all(funs(mean(.)))
```

` `

The difference in the mean of log of duration of unemployment between treated and control groups is `r round(ate_diff_mean, 3)`. This implies that those that receive the treatment spend less time being unemployed than those who don't get the treatment. 

Controlling by observable characteristics of the individuals, the log of duration of unemployment is `r round(abs(ate_ols$coefficients[2]), 3)` smaller for the individuals that receive the treatment. Once we control by our vector of observables `x`, the effect of the treatment is `r round(abs(ate_diff_mean) - abs(ate_ols$coefficients[2]), 3)` smaller than when comparing using the difference of means (without controls). 

Ideally, randomization should balance the distribution of covariates among treated and untreated. One way to check if this is true is by calculating the sample mean difference in covariates between treatment and control groups. The balance is in general quite good but some characteristics are still not very well balanced (we could test if the differences are significantly different from zero). This can explain the difference in the ATE by the two previous methods.

` `

## (b) One way to evaluate if the randomization is sucessful is to test the significance of $\theta_0$ in a Probit specification of the propensity score $p(x)=\Phi(x'\theta_0)$. Run such a test and interpret the results. Discuss the type of test, critical value, etc.

` `

```{r part1_b}
# Probit model estimation
penn4_ps <- glm(
  as.formula(paste0("tg ~", paste0(x, collapse = " + "))),
  family = binomial(link = "probit"), 
  data = penn4)
summary(penn4_ps)

# Extract coefficients
penn4_ps_coef <- coef(penn4_ps)

# Extract variance-covariance matrix
penn4_ps_sigma <- vcov(penn4_ps)

# Testing joint significance (Wald test)
penn4_ps_wt <- wald.test(Sigma = penn4_ps_sigma, b = penn4_ps_coef, Terms = 2:15)
penn4_ps_wt
```

` `

To test if randomization was done correctly we can test the joint significance of the covariates on the probability of being treated. If the assignment of the treatment was truly random, no observable characteristic should be significant. To that end, we can apply a Wald test to the vector of coefficients $\theta_0$. 

The test result indicates that we can not reject the null of all coefficients being simultaneously equal to zero for a significance level of 0.05. So far, the evidence is not strong against the success of the randomization process. 

` `

## (c) Estimate the ATE by DML based on Lasso.

` ` 

```{r part1_c}
set.seed(123)

# Double Debiased Machine Learning function with lasso (DML)
b_DML <- function(Y,X,D){
  DML1 <- cv.glmnet(X, Y, alpha = 1)
  yhat <- predict(DML1, X)
  res1 <- Y - yhat
  DML2 <- cv.glmnet(X, D, alpha = 1)
  Dhat <- predict(DML2, X)
  res2 <- D - Dhat
  DML <- lm(res1 ~ 0 + res2)
  DML_coef <- as.numeric(coef(DML))
  DML_se <- as.numeric(summary(DML)$coefficients["res2","Std. Error"])
  b_DML <- c(DML_coef, DML_se)
  return(b_DML)
}

# Calculating ATE using DML
ate_dml <- b_DML(penn4$l_inuidur1, as.matrix(penn4[, x]), penn4$tg)
ate_dml
```

` `

The ATE obtained using DML technique with lasso is `r round(ate_dml[1], 3)`.

` `

## (d) Construct 95% CI for the ATE using the previous estimates.

` `

```{r part1_d}
# CI ATE from difference in means

#Lower
ate_diff_mean_ci_l <- ate_diff_mean - qnorm(0.975) * sqrt(penn4_summary$sd[1]^2 / penn4_summary$n[1] + penn4_summary$sd[2]^2 / penn4_summary$n[2])
#Upper
ate_diff_mean_ci_u <- ate_diff_mean + qnorm(0.975) * sqrt(penn4_summary$sd[2]^2 / penn4_summary$n[2] + penn4_summary$sd[2]^2 / penn4_summary$n[2])

ate_diff_mean_ci <- c(ate_diff_mean_ci_l, ate_diff_mean_ci_u)
ate_diff_mean_ci

# CI ATE from OLS

#Lower
ate_ols_ci_l <- as.numeric(ate_ols$coefficients[2] - qnorm(0.975) * summary(ate_ols)$coefficients["tg","Std. Error"])
#Upper
ate_ols_ci_u <- as.numeric(ate_ols$coefficients[2] + qnorm(0.975) * summary(ate_ols)$coefficients["tg","Std. Error"])

ate_ols_ci <- c(ate_ols_ci_l, ate_ols_ci_u)
ate_ols_ci

# CI ATE from DML

#Lower
ate_dml_ci_l <- ate_dml[1] - qnorm(0.975) * ate_dml[2]
#Upper
ate_dml_ci_u <- ate_dml[1] + qnorm(0.975) * ate_dml[2]

ate_dml_ci <- c(ate_dml_ci_l, ate_dml_ci_u)
ate_dml_ci
```
` ` 

Each confidence interval is given below:

  - Difference in means: $[$`r round(ate_diff_mean_ci[1], 4)`, `r round(ate_diff_mean_ci[2], 4)`$]$ 
  
  - OLS: $[$`r round(ate_ols_ci[1], 4)`, `r round(ate_ols_ci[2], 4)`$]$
  
  - DML: $[$`r round(ate_dml_ci[1], 4)`, `r round(ate_dml_ci[2], 4)`$]$
  
` `

# Part 2

## (a) Suppose now that (1) holds with "$\varepsilon_i$ independent of $D_i$ and $X_i$. What is the corresponding quantile regression of $Y$ given $X_i$, $D_i$ associated to (1)? How would you test (visually) the previous independence assumption using quantile regression?

` ` 

When using the model $Y_i = \alpha_0 + \tau_0 D_i + \beta'_0 X_i + \epsilon_i$, if we assume independency of $\epsilon_i$, we see that the function $\alpha_0 + \tau_0 D_i + \beta'_0 X_i$ corresponds to the conditional mean effect of the treatment. But when the mean and the median of a distribution do not coincide, the median may be more appropriate to capture the central tendency of the distribution, so the corresponding quantile regression of $Y$ given $X_i, D_i$ is $Y_i = \alpha_0^{(q)} + \tau_0^{(q)} D_i + {\beta'}_0^{(q)} X_i + \epsilon_i^{(q)}$, where $0<q<1$ indicates the proportion of the population having scores below the quantile $q$. If we assume a symmetric distribution, we can consider $q = 0.5$.

To test the initial independence assumption, is enough to plot the coefficients relative to different quantiles and see if they are constant or not. If they are constant, we can say that the assumption is correct, but if not, it means that is necessary to use the quantile regression.

` `

## (b) Estimate for a grid of 300 points between [0.2,0.8] the quantile coeficients of $Y$ given $D$ and $X$ and plot the coeficients (similarly to Figure 1 in Koenker and Xiao). Is the independence of the error a realistic assumption in this application? Interpret the coeficient of treatment in the quantile regressions.

` `

In a randomized experiment, the independence of the error is a valid assumption. In general, perfect randomization is difficult to achieve, which is why checking the robustness of the randomization process becomes relevant. In part 1, we couldn't find strong evidence against the random assignment of the treatment, which is why seems like a plausible assumption.

` `

```{r part2_b, results = 'hide', fig.cap = "Quantile regression coefficient estimations", fig.width = 10, fig.height = 9}
# Generate sequence for tau parameter
alphagrid <- c(seq(0.2, 0.8, length.out = 300))

# Run quantile regression for grid of tau
quant <- rq(as.formula(paste0("l_inuidur1 ~ tg +", paste0(x, collapse = " + "))), data = penn4, tau = alphagrid)

# Plot estimated coefficients (function of tau)
plot(summary(quant), ols = FALSE)
```

` `

The unemployment extension benefit has a U shape, meaning that as we move along the income distribution, the treatment has a higher impact on workers, being between the third and sixth quantile where we can observe the highest effect. For workers above the sixth decile of income, the treatment is still significant but it has a smaller impact, until it becomes zero for the highest income workers. 

` `

## (c) Explain how you would compute bootstrap standard errors for the QRE estimator. What is the advantage of doing bootstrap for computing standard errors over estimating the asymptotic variance for quantile regression? Compute a bootstrap confidence interval for the median treatment effect.

```{r part2_c, results = 'hide'}
# Bootstrap estimation of coefficients for tau = 0.5 using 200 replications
boot <- boot.rq(penn4[, c("tg", x)], penn4$l_inuidur1, tau = 0.5, R = 200)

# Extracting tg vector 
boot.tg <- boot$B[, 1]

# Confidence interval for the median treatment effect
lower.ci <- mean(boot.tg) - abs(quantile(boot.tg, 0.025)) * sd(boot.tg)
upper.ci <- mean(boot.tg) + abs(quantile(boot.tg, 0.975)) * sd(boot.tg)
(ci <- c(lower.ci, upper.ci))
```

` `

## (d) How would you allow for quantile treatment effects that vary with some covariates? Read Escanciano and Goh (2019), and investigate how treatment effects may vary by age using your preferred method.

` ` 

If we need quantile treatment effects that vary with some covariates, is equivalent to allow for the heterogeneity in treatment effects, in which we include interaction terms in a regression model to adapt for it. In our case, we include an interaction between a dummy variable of age ($1$ if younger than 35 years old and $0$ otherwise at the time of the experiment) and treatment indicator and estimate differences in treatment effects between these age groups.
After that, is possible to plot the estimated differences in treatment effects for each quantile, simmilar to above. The shaded area indicates the union of $90%$ confidence intervals for the estimated difference in treatment effects at each quantile. We conclude that workers younger than 35 tend to exit unemployment as a result of the treatment significantly more quickly than workers 35 and older for nearly all quantiles in the interval $[0.50, 0.80]$, as seen in Escanciano and Goh (2019).

` `

# Part 3

` `

Suppose we observe $X_1,...,X_n$ from a U$[0,\theta]$.

## (a) NTS: $\hat{\theta}_n = max\{X_1,...,X_n\}$

The likelihood function is: $$l(\theta) = \theta^{-n} 1_{(X_{(n)},\infty)}(\theta)$$ 
Since $l(\theta)$ is strictly decreasing on $(X_{(n)},\infty)$ and is equal zero on $(0, X_{(n)})$, a unique maximum of $l(\theta)$. Hence, MLE of $\theta$ is the largest order statistic $X_{(n)}$.

## (b) NTS: $T_n(\theta) = n(\hat{\theta}_n - \theta) \rightarrow^d -\theta Z$, where $Z \sim exp(1)$

$$P(n(\hat{\theta}_n - \theta) \leq x) = P (\hat{\theta} \leq \dfrac{x}{n} + \theta) = P(X_n \leq \dfrac{x}{n} + \theta) = \bigcap_{i=1}^n \{X_i \leq \dfrac{x}{n} + \theta\} = T_n(\theta) \leq x$$
$$\bigcap_{i=1}^n \{X_i \leq \dfrac{x}{n} + \theta\} = \prod_{i=1}^{n} P(X_i \leq \dfrac{x}{n} + \theta) = \Big(\dfrac{1}{\theta}(\theta+\dfrac{x}{n})\Big)^n = \Big( 1 + \dfrac{\frac{x}{\theta}}{n}\Big)^n = e^{\frac{x}{\theta}}$$

Hence, $T_n(\theta) = e^{\frac{x}{\theta}} \rightarrow^d -\theta Z$, where $Z \sim exp(1)$.

## (c) NTS: $Pr\Big(n(\hat{\theta}^*_n - \hat{\theta}_n) = 0 \Big) = 1 - (1-\frac{1}{n})^n \rightarrow 1-e^{-1} >0$

  (i) $(1-\frac{1}{n})^n \rightarrow e^{-1}$
  (ii) $Pr\Big(n(\hat{\theta}^*_n - \hat{\theta}_n) = 0 \Big) = Pr\Big(\hat{\theta}_n = \theta_n\Big) = Pr\Big(X_{(n)}^*= X_{(n)}\Big)$
    $Pr \Big( X^* = X_{(n)} \; or \; ... \; or \; X_n^* = X_{(n)} \Big) = Pr \Big( \bigcup_{i=1}^n \big\{X_i^* = X_{(n)} \big\} \Big) = 1 - Pr\Big( \bigcap_{i=1}^n \big\{X_i^* \neq X_{(n)} \big\} \Big)$
  (iii) $Pr\Big( \bigcap_{i=1}^n \big\{X_i^* \neq X_{(n)} \big\} \Big) = \prod_{i=1}^{n} Pr\Big( X_i^* \neq X_{(n)} \Big) = Pr\Big( X_i^* \neq X_{(n)} \Big)^n = 1 - Pr\Big( X_i^* = X_{(n)} \Big)^n = (1-\frac{1}{n})^n$
  (iv) Putting everything together: $1 - (1 - \frac{1}{n})^n \rightarrow 1 - e^{-1} >0$ 
  
## (d) 

$Pr\big(n(\hat{\theta}_n - \theta)=0\big) = Pr(\hat{\theta} = \theta) = Pr\Big(X_{(n)}= \theta\Big) = 0$, since $X_{(n)}$ is continuously distributed.

Hence $\lim_{n\to\infty} Pr\Big(n(\hat{\theta}_n - \theta ) = 0 \Big) \neq \lim_{n\to\infty} Pr\Big(n(\hat{\theta}^*_n - \hat{\theta}_n) = 0 \Big)$

` `

# Part 4

` `

```{r, eval = FALSE}
# Calling functions
source("../Assignment_4/Q4.R")

# Parameters
reps <- 1000
m <- c(100, 300)
t <- c(-2, -1, -0.5, -0.25, 0, 0.25, 0.5, 1, 2)
g <- 2
b <- 0.25
s <- 1.5
k <- 2

for (n in m) {
  for (tau in t) {
    b_MC <- MC_unif(reps,n,tau,g,b,s,k)
  }
}
```

` `

In order to compare the empirical size and power performance of the robust LM and the Wald test, we did a Monte Carlo simulation with 1000 repetition of the DML ATE.
On the Wald test it is necessary to compute the unrestricted model to evaluate the effect, find the coefficients and the variance-covariance matrix, and apply the `wald.test` function. On the other hand, for the LM test, it is necessary to compute the restricted model to find the test values from `lmtest` (defined in file `Q4.R`).

After finding for one of the $\tau_0$, a loop is created to calculate for each of the $\tau$, and for each number of observations (100 and 300).
A table is plotted to make it easier to compare the test values.
We also know that asymptoticaly, both tests are equivalent, so with a large value of repetitions, it is indifferent between choosing any of them.
Remark: Lm test is not consistent under heteroskedasticity that is why we have used formula which is robust to heteroskedasticty.